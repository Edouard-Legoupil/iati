---
title: "AI Analysis"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r development, include=FALSE}
library(testthat)
```

```{r development-load}
# Load already included functions if relevant
pkgload::load_all(export_all = FALSE)
```

# Using Language Models to build data stories

## clean_llm_response

```{r function-clean_llm_response}
#' Clean LLM Response
#'
#' Remove thinking tags, markdown, and other artifacts from LLM responses
#' Useful for Local Open Source Reasoning Small Language Model
#'
#' @param response Character string from LLM response
#' @param keep_punctuation boolean
#' @return Cleaned character string
#' @export
clean_llm_response <- function(response, keep_punctuation = TRUE) {
  if (!is.character(response)) return(response)
  
  # Split into lines
  lines <- strsplit(response, "\n")[[1]]
  
  # Find thinking tag boundaries
  think_start <- which(grepl("<think>", lines, ignore.case = TRUE))
  think_end <- which(grepl("</think>", lines, ignore.case = TRUE))
  
  # Remove ALL lines between thinking tags (inclusive)
  if (length(think_start) > 0 && length(think_end) > 0) {
    remove_indices <- integer(0)
    for (i in seq_along(think_start)) {
      if (think_start[i] <= think_end[i]) {
        remove_indices <- c(remove_indices, think_start[i]:think_end[i])
      }
    }
    if (length(remove_indices) > 0) {
      lines <- lines[-remove_indices]
    }
  }
  
  # Combine back
  response <- paste(lines, collapse = " ")
  
  # Remove all non-ASCII characters
  response <- gsub("[^\x20-\x7E]", "", response)
  
  # Remove common LLM artifacts and introductory phrases
  response <- gsub("^(Sure|Certainly|Okay|Here|First).*?(:|\\.)\\s*", "", response, ignore.case = TRUE)
  response <- gsub("^(As an AI|I am an AI|I'm a).*?\\.\\s*", "", response, ignore.case = TRUE)
  response <- gsub("^(My role includes|This includes).*?\\.\\s*", "", response, ignore.case = TRUE)
  response <- gsub("^(Aligning with constraints).*?\\.\\s*", "", response, ignore.case = TRUE)
  
  # Remove markdown formatting
  response <- gsub("\\*\\*|\\*|__|_", "", response)
  response <- gsub("\\[.*?\\]\\(.*?\\)", "", response)
  response <- gsub("#+\\s*", "", response)
  response <- gsub("`{1,3}", "", response)
  
  # Remove excessive whitespace
  response <- gsub("\\s+", " ", response)
  response <- trimws(response)
  
  # If empty after cleaning, provide fallback
  if (nchar(response) == 0) {
    return("Unable to generate story from this visualization.")
  }
  
  return(response)
}
```
  
```{r example-clean_llm_response}
response <- "<think>
First, I'm a humanitarian data visualization expert. My role includes extracting insights 
from visualizations, creating accessible narratives, highlighting patterns relevant to aid
efforts, using clear language with emotional resonance.
Aligning with constraints: Use plain language, be concise and impactful. Don't rehash 
every detail; build narrative depth around 2 key insights maximum in under 30 tokens.
</think>
This visualization tracks a relationship potentially critical for humanitarian logistics: 
higher fuel consumption versus increased weight. 车辆设计"
clean_llm_response(response)
```
  
```{r tests-clean_llm_response}
test_that("clean_llm_response works", {
  expect_true(inherits(clean_llm_response, "function")) 
})
```
  

## generate_plot_story

```{r function-generate_plot_story}
#' Generate Humanitarian Data Story from ggplot
#'
#' This function takes a ggplot2 object and generates a storytelling narrative
#' focused on humanitarian insights. It uses the \{ellmer\} package to call
#' a large or small language model from a supported provider.
#' 
#' If you do not have API keys or need to work offline, simply get ollama and 
#' look at top reasoning models - https://ollama.com/search?c=thinking 
#'
#' Setup:
#' 1. Install \{ellmer\}: `install.packages("ellmer")`
#' 2. Set your API key in your environment. For Azure OpenAI, use the standard
#'    OpenAI key variable:
#'    `Sys.setenv(OPENAI_API_KEY = "<YOUR_AZURE_OPENAI_KEY>")`
#' 3. When using Azure, set `provider = "azure"` and provide the env variables
#'   `Sys.setenv(AZURE_OPENAI_ENDPOINT = "<YOUR_AZURE_ENDPOINT>")`
#'   `Sys.setenv(AZURE_OPENAI_API_VERSION = "<YOUR_AZURE_OPENAI_API_VERSION>")`
#'   The best place to set this is in .Renviron, which you can easily edit by
#'    calling `usethis::edit_r_environ()`
#'
#' @param plot A `ggplot` object from ggplot2.
#' @param max_tokens Maximum number of tokens (approximate) 
#' for the narrative (default = 30).
#' @param provider Optional character string specifying the provider. Options 
#' include:
#'   `"openai"`, `"gemini"`, `"anthropic"`, `"ollama"`, `"azure"`. If `NULL`, 
#'   auto-detect from environment keys.
#' @param model Optional character string specifying the model name. For Azure,
#'    this is typically the deployment name. If `NULL`, a default model for the
#'    chosen provider will be used.
#' @param clean_response Logical. Whether to clean the response by removing
#'   thinking tags and other artifacts (default = TRUE).
#'
#' @return A character string containing a storytelling narrative focused on
#'  humanitarian data.
#'
#' @importFrom ggplot2 ggplot_build
#' @importFrom dplyr mutate_if
#' @importFrom utils capture.output head
#' @importFrom ellmer chat_openai chat_google_gemini chat_anthropic chat_ollama chat_azure_openai
#' @export
generate_plot_story <- function(plot, 
                                max_tokens = 30,
                                provider = NULL,
                                model = NULL,
                                clean_response = TRUE) {
  
  # Extract plot data (first layer) and truncate
  plot_data <- ggplot2::ggplot_build(plot)$data[[1]] |>
    dplyr::mutate_if(is.numeric, round, 2) |>
    head(30)
  plot_data_text <- capture.output(print(plot_data))
  
  # Extract title, subtitle, caption
  labels <- plot$labels
  title    <- if (!is.null(labels$title))    labels$title    else ""
  subtitle <- if (!is.null(labels$subtitle)) labels$subtitle else ""
  caption  <- if (!is.null(labels$caption))  labels$caption  else ""
  
  # Extract mapping aesthetics
  mapping_info <- if (!is.null(plot$mapping)) {
    mapping_text <- capture.output(print(plot$mapping))
    paste("Global mapping:", paste(mapping_text, collapse = " "))
  } else {
    "No global mapping defined"
  }

  # Extract layer-specific mappings
  layer_mappings <- sapply(plot$layers, function(layer) {
    if (!is.null(layer$mapping)) {
      paste("Layer mapping:", capture.output(print(layer$mapping)))
    } else {
      "No layer-specific mapping"
    }
  })
  layer_mappings_text <- paste(unique(layer_mappings), collapse = "; ")
  
  # Detect scales and transformations
  scale_info <- if (!is.null(plot$scales$scales) && length(plot$scales$scales) > 0) {
    scale_types <- sapply(plot$scales$scales, function(scale) {
      paste(class(scale)[1], "scale (", scale$aesthetics[1], ")")
    })
    paste("Scale transformations:", paste(unique(scale_types), collapse = ", "))
  } else {
    "No custom scale transformations detected"
  }
  
  # Detect geoms used
  geoms <- unique(sapply(plot$layers, function(layer) class(layer$geom)[1]))
  geoms_text <- paste(geoms, collapse = ", ")
  
  system_prompt <- paste0(
    
    "You are a Senior Humanitarian Data Storyteller for UNHCR. ",
    "Your goal is to influence decision-makers by turning data into a 'Cause and Effect' narrative.\n\n",
    "Your role is to: \n",
    "1. Extract, from a given dataset and its visualization parameters, key operational and fund raising insights with quotable and specific data points \n",
    "2. Create compelling, accessible narratives tailored for a humanitarian proposal writer audience \n",
    "3. Highlight patterns, trends, and implications to consider when looking for funding opportunities and designing new proposals or evaluation studies \n",
    "4. Adapt interpretation depth based on available data and context \n",
    "5. Use clear, plain, WCAG-compliant language with appropriate practical recommendations \n\n",
    
    "CRITICAL INSTRUCTIONS:\n",
    "- DO NOT include any thinking tags like <think> or </think> in your response\n",
    "- DO NOT use markdown formatting, asterisks, or special characters\n",
    "- Provide ONLY the final narrative text\n",
    "- Your response should be clean plain text ready for display\n\n"
    
  )
  
  # Build enhanced prompt
  prompt <- paste0(
    "VISUALIZATION CONTEXT:\n",
    "Consider what each mapped variable represents and any data transformations applied.\n\n",
    "Title: ", title, "\n",
    "Subtitle: ", subtitle, "\n", 
    "Caption: ", caption, "\n",
    "Chart type(s): ", geoms_text, "\n",
    "Data mappings: ", mapping_info, "\n",
    "Layer mappings: ", layer_mappings_text, "\n", 
    "Scales: ", scale_info, "\n\n",
    
    "Use the following DATA:\n", paste(plot_data_text, collapse = "\n"), "\n\n",
    
    "TASK: Create a fund raising analysis using the provided data and visualization parameters.\n",
    
    "ADAPTIVE CONSTRAINTS:\n",
    "- Important! Maximum length for the generated output: ", max_tokens, " tokens\n",
    "- Prioritize clarity over completeness\n",
    "- Focus on 2-3 key insights if space is limited\n",
    "- Use concise but impactful language\n",
    "- IMPORTANT: Provide only clean text output without any thinking tags or markdown\n\n"
    
  )
  
  # Auto-detect provider if not specified
  if (is.null(provider)) {
    # Check for Azure-specific environment variables
    if (!is.na(Sys.getenv("AZURE_OPENAI_ENDPOINT", unset = NA_character_)) &&
        !is.na(Sys.getenv("AZURE_OPENAI_API_KEY", unset = NA_character_))) {
      provider <- "azure"
    } else if (!is.na(Sys.getenv("OPENAI_API_KEY", unset = NA_character_))) {
      provider <- "openai"
    } else if (!is.na(Sys.getenv("GEMINI_API_KEY", unset = NA_character_))) {
      provider <- "gemini"
    } else if (!is.na(Sys.getenv("ANTHROPIC_API_KEY", unset = NA_character_))) {
      provider <- "anthropic"
    } else {
      stop("No supported API key found. Set AZURE_OPENAI_ENDPOINT/KEY, OPENAI_API_KEY, GEMINI_API_KEY,
           ANTHROPIC_API_KEY, or install and set it to a local OLLAMA")
    }
  }
  
  provider <- tolower(provider)
  
  # Set default models if not provided
  if (is.null(model)) {
    model <- switch(
      provider,
      openai = "gpt-4o-mini",
      gemini = "gemini-2.5-flash",
      anthropic = "claude-3-5-sonnet-20241022",
      ollama = "deepseek-r1",
      azure = "gpt-4", # Placeholder: Must be a valid deployment name
      stop("Invalid provider specified. Choose from 
           'openai', 'gemini', 'anthropic', 'ollama', 'azure'.")
    )
  }
  
  # Initialize chat object
  chat <- switch(
    provider,
    openai = ellmer::chat_openai(model = model, system_prompt = system_prompt),
    
    # Azure OpenAI using the dedicated function and explicit environment variable checks
    azure = {
      # Fetch required environment variables
      azure_key <- Sys.getenv("AZURE_OPENAI_API_KEY")
      azure_endpoint <- Sys.getenv("AZURE_OPENAI_ENDPOINT")
      azure_version <- Sys.getenv("AZURE_OPENAI_API_VERSION")
      
      # VALIDATION: Check if any required variable is unset (returns "")
      if (azure_key == "" || azure_endpoint == "" || azure_version == "") {
        stop("For 'azure' provider, AZURE_OPENAI_API_KEY, AZURE_OPENAI_ENDPOINT, and AZURE_OPENAI_VERSION environment variables must all be set correctly in the R session.")
      }
      
      # Initialize chat object, passing the validated environment variables
      ellmer::chat_azure_openai(
        system_prompt = system_prompt,
        model = model,
        api_version = azure_version,
        endpoint = azure_endpoint,
        api_key = azure_key
      )
    },
    
    gemini = ellmer::chat_google_gemini(
      model = model,
      system_prompt = system_prompt,
      base_url = "https://generativelanguage.googleapis.com/v1beta/",
      api_key = Sys.getenv("GEMINI_API_KEY")),
    
    anthropic = ellmer::chat_anthropic(
      model = model, 
      system_prompt = system_prompt),
    
    ollama = ellmer::chat_ollama(
      model = model,
      system_prompt = system_prompt),
    
    stop(
      "Invalid provider specified. Choose from
         'openai', 'gemini', 'anthropic', 'ollama', 'azure'."
    )
  )
  
  # Send prompt and get response
  response <- chat$chat(prompt)
  
  # Clean response if requested
  if (clean_response) {
    response <- clean_llm_response(response)
  }
  
  return(response)
}

```
  
```{r example-generate_plot_story, warning=FALSE, fig.retina = 2, fig.width = 12, fig.asp = 0.818, fig.align = "center", out.width = "90%"}

library(ggplot2)
p <- ggplot(mtcars, aes(x = wt, y = mpg)) +
   geom_point() +
    unhcrthemes::theme_unhcr(grid = "Y", axis = "X", axis_title = FALSE) +
   labs(title = "Vehicle Efficiency",
        subtitle = "Fuel consumption vs weight",
        caption = "Source: mtcars dataset")

generate_plot_story(p, provider = "ollama", model = "deepseek-r1")

story <- generate_plot_story(p, provider = "azure", model = "gpt-4.1-mini", max_tokens = 30)
# To use as subtitle:
p + ggplot2::labs(subtitle = story)
```
  
```{r tests-generate_plot_story}
test_that("generate_plot_story works", {
  expect_true(inherits(generate_plot_story, "function")) 
})
```


# slugify

```{r function-slugify}
#' slugify
#' 
#' Convert string to slug
#' 
#' @param x the string
#' 
#' @return slugified string
#' @importFrom stringr str_to_lower str_replace_all str_remove_all str_squish
#' @importFrom stringi stri_enc_toutf8
#' @export
slugify <- function(x) {
  x |>
    stringi::stri_enc_toutf8() |>
    # Remove extra whitespace first
    stringr::str_squish() |>
    # Convert to lowercase
    stringr::str_to_lower() |>
    # Replace accented characters with their ASCII equivalents
    stringr::str_replace_all("[àáâãäå]", "a") |>
    stringr::str_replace_all("[èéêë]", "e") |>
    stringr::str_replace_all("[ìíîï]", "i") |>
    stringr::str_replace_all("[òóôõöø]", "o") |>
    stringr::str_replace_all("[ùúûü]", "u") |>
    stringr::str_replace_all("[ñ]", "n") |>
    stringr::str_replace_all("[ç]", "c") |>
    stringr::str_replace_all("[ýÿ]", "y") |>
    stringr::str_replace_all("[ž]", "z") |>
    # Remove all non-alphanumeric characters except spaces, underscores, AND hyphens
    stringr::str_replace_all("[^a-z0-9\\s_-]", "") |>
    # Replace spaces and underscores with hyphens
    stringr::str_replace_all("[\\s_]+", "-") |>
    # Remove consecutive hyphens
    stringr::str_replace_all("-+", "-") |>
    # Remove leading/trailing hyphens
    stringr::str_remove_all("^-|-$")
}
```
  
```{r example-slugify}
strings <- c("Café au Lait", "Niño Español", "Data_Science_Project", "--test--string--")
slugify(strings)
```
  
```{r tests-slugify}
test_that("slugify works", {
  expect_true(inherits(slugify, "function")) 
})
```
  

```{r development-inflate, eval=FALSE}
# Run but keep eval=FALSE to avoid infinite loop
# Execute in the console directly
fusen::inflate(flat_file = "dev/dev_ai_analysis.Rmd", vignette_name = "AI analysis")
```

